{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194ed95e",
   "metadata": {},
   "source": [
    "### Universidad del Valle de Guatemala\n",
    "### Deep Learning\n",
    "### Laboratorio 7\n",
    "### Andres Quezada 21085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8dc36e0473657d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:33.480262Z",
     "start_time": "2024-10-04T06:16:32.408536Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e450f2",
   "metadata": {},
   "source": [
    "**Layer Normalization**\n",
    "\n",
    "Esta clase implementa la normalización de capas, que es necesario en los transformadores para estabilizar el entrenamiento y mejorar la convergencia, se encarga de ajustar cada capa de activaciones al aprender los parámetros alpha y  bias. Estos parámetros se actualizan durante el entrenamiento para optimizar las activaciones de las capas. Por eso es que ambos son learnable parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1f583d3356e6ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:36.794959Z",
     "start_time": "2024-10-04T06:16:36.790425Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps # epsilon es un valor pequeño para evitar la división por cero\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True) # mean y std se calculan en la última dimensión\n",
    "        std = x.std(dim = -1, keepdim = True)  # keepdim = True para mantener la dimensión de la media y la desviación está\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "    # Regresa la normalización de x con alpha y bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09987dd",
   "metadata": {},
   "source": [
    "**Feed Forward Block**\n",
    "\n",
    "Aquí se define la red feed-forward que se aplica después de la atención en cada capa del codificador y del decodificador. Es una red completamente conectada con dos capas lineales, una activación ReLU, y una capa de dropout para reducir el sobreajuste. Este bloque es importante para transformar las representaciones aprendidas por las capas de atención y generar las siguientes representaciones más complejas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c41cd1eeba9f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:46.526151Z",
     "start_time": "2024-10-04T06:16:46.514438Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # primera capa lineal con d_model entradas y d_ff salidas\n",
    "        self.dropout = nn.Dropout(dropout) # dropout para regularización\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)  # segunda capa lineal con d_ff entradas y d_model salidas\n",
    "\n",
    "    def forward(self, x):\n",
    "        # regresa la salida de la segunda capa lineal después de aplicar dropout y relu a la primera capa lineal\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089c7ad",
   "metadata": {},
   "source": [
    "**Input Embeddings**\n",
    "\n",
    "En esta clase se conviertem los tokens de entrada en las representaciones vectoriales (embeddings) que se utilizarán para procesar la secuencia de entrada. Este paso es necesario en los modelos de lenguaje, ya que transforman palabras o tokens discretos en vectores que permiten capturar relaciones semánticas entre ellos. Permitiendo la búsqueda de atención en oraciones y textos largos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daab70ff78271aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:50.226749Z",
     "start_time": "2024-10-04T06:16:50.221461Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # tamaño del embedding\n",
    "        self.vocab_size = vocab_size # tamaño del vocabulario\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # embedding de tamaño vocab_size x d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "    # regresa el embedding de x multiplicado por la raíz cuadrada de d_model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11407329",
   "metadata": {},
   "source": [
    "**Positional Encoding**\n",
    "\n",
    "Aquí se implementa el Positional Encoding, que es necesario en la arquitectura de transformadores ya que estos modelos no tienen una estructura secuencial como las redes recurrentes u otras que hemos visto en clase. Entonces, el transformador no sabe cuál es el orden de las palabras de la secuencia, y para lograr que sepa, hay que introducir información sobre el orden, por eso se asigna una representación única a cada posición de la secuencia.\n",
    "\n",
    "Se utilizan funciones seno y coseno para generar patrones periódicos que varían según la posición de la palabra en la secuencia, permitiendo que el modelo distinga el orden relativo entre las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7ab738d06eb466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:53.785818Z",
     "start_time": "2024-10-04T06:16:53.776620Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # tamaño del embedding\n",
    "        self.seq_len = seq_len # longitud de la secuencia\n",
    "        self.dropout = nn.Dropout(dropout) # dropout para regularización\n",
    "        \n",
    "        pe = torch.zeros(seq_len, d_model) # inicializa pe con ceros\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # posición de la secuencia\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # término de división\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # seno en las posiciones pares\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # coseno en las posiciones impares\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "        self.register_buffer('pe', pe) # registra pe como buffer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n",
    "    # regresa x más el positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed03a3",
   "metadata": {},
   "source": [
    "**Residual Connection**\n",
    "\n",
    "Aquí se implementan las conexiones que permiten que las entradas originales se sumen a las salidas de las capas posteriores. Esto ayuda a evitar el problema del vanishing gradient, facilitando el flujo de gradientes durante el entrenamiento y mejorando la estabilidad de las redes profundas. El dropout y la normalización de capas también se aplican para evitar el sobreajuste y mantener la estabilidad durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b390e443e3431509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:56.169338Z",
     "start_time": "2024-10-04T06:16:56.160187Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Aplica dropout para evitar el sobreajuste\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Normalización de capas para estabilizar el entrenamiento\n",
    "        self.norm = LayerNormalization(features)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        # La salida de la subcapa normalizada se suma a la entrada original \n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be0d29",
   "metadata": {},
   "source": [
    "**Multi Head Attention Block**\n",
    "\n",
    "Aquí se implementa el multi-head attention, que permite que el modelo aprenda diferentes relaciones entre los elementos de la secuencia simultáneamente a través de diferentes puntos de atención. Cada uno de estos puntos proyecta los queries, keys y values en subespacios diferentes, y luego los combina para formar una representación final enriquecida con toda la información.\n",
    "\n",
    "El mecanismo de atención permite que el modelo preste atención a diferentes partes de la secuencia de entrada, ponderando la importancia de cada palabra en función de otras. Mejorando la capacidad del modelo para capturar múltiples tipos de relaciones entre las palabras o tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f60a40ee5a93f250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:16:58.474673Z",
     "start_time": "2024-10-04T06:16:58.461541Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        # Verifica que el número de dimensiones (d_model) sea divisible por el número de cabezas (h)\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        \n",
    "        # Tamaño de las proyecciones por cabeza\n",
    "        self.d_k = d_model // h  \n",
    "        # Definición de las capas lineales para proyecciones de queries, keys, values y la salida\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        # Calcula las puntuaciones de atención escaladas\n",
    "        d_k = query.shape[-1]\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # Aplica la máscara, si existe, para ignorar ciertas posiciones\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        \n",
    "        # Se aplica softmax para normalizar las puntuaciones en probabilidades\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        \n",
    "        # Aplica dropout para evitar el sobreajuste en las puntuaciones de atención\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        \n",
    "        # Calcula la salida de la atención multiplicando por los valores (values)\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # Aplica las proyecciones lineales para obtener los queries, keys y values\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        # Reorganiza los tensores para aplicar la atención múltiple (multi-head attention)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calcula la atención con múltiples cabezas\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Reorganiza las salidas y aplica la proyección final\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        # Proyección final de la salida combinada de las cabezas de atención\n",
    "        return self.w_o(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f14be",
   "metadata": {},
   "source": [
    "**Encoder Block**\n",
    "\n",
    "Aquí empieza la fase codificadora del transformador, se aplica atención múltiple a la entrada, seguido de un bloque feed-forward. Cada operación dentro del bloque (la self-attention y el feed-forward) se acompaña de una residual connection, lo que ayuda a preservar la información de las capas anteriores, evitando la degradación de la señal. También se aplica normalización de capas y dropout para estabilizar y regularizar el entrenamiento.\n",
    "\n",
    "El EncoderBlock es un bloque clave en la arquitectura del transformador, ya que permite procesar una secuencia de entrada para generar representaciones útiles que capturan tanto dependencias locales como globales dentro de la secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92cdb1da2c2c2920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:04.850156Z",
     "start_time": "2024-10-04T06:17:04.844375Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Bloque de autoatención múltiple\n",
    "        self.self_attention_block = self_attention_block\n",
    "        # Bloque feed-forward que sigue a la atención\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        # Se crean dos conexiones residuales: una para la atención y otra para el feed-forward\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # Aplica la primera conexión residual con el bloque de autoatención\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        # Aplica la segunda conexión residual con el bloque feed-forward\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438a2e4",
   "metadata": {},
   "source": [
    "**Encoder**\n",
    "\n",
    "Esta clase representa el conjunto completo de capas del codificador en un transformador. Un transformador codificador está formado por múltiples capas, donde cada una de estas capas aplica una combinación de autoatención y bloques feed-forward con conexiones residuales. En esta clase, las capas se procesan de manera secuencial, y después de pasar por todas ellas, se aplica una normalización de capa final para estabilizar las representaciones generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d52695c77abc1cbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:06.721751Z",
     "start_time": "2024-10-04T06:17:06.715885Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        # Las capas del codificador, normalmente una lista de EncoderBlocks\n",
    "        self.layers = layers\n",
    "        # Normalización de capa al final de todas las capas del codificador\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Pasa la entrada a través de cada capa del codificador secuencialmente\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        # Aplica normalización de capa al resultado final después de todas las capas\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe2b0b1",
   "metadata": {},
   "source": [
    "**Decoder Block**\n",
    "\n",
    "Y el complemeto del bloque anterior, está aqui en el bloque del decodificador en el transformador. El decodificador es responsable de generar las salidas basadas tanto en las representaciones del codificador como en las secuencias de salida generadas hasta el momento. \n",
    "\n",
    "Este bloque realiza tres operaciones principales:\n",
    "\n",
    "1. Autoatención sobre la secuencia de salida generada hasta el momento.\n",
    "\n",
    "2. Atención cruzada sobre las salidas del codificador, permitiendo que el decodificador se enfoque en diferentes partes de la secuencia de entrada.\n",
    "\n",
    "3. Bloque feed-forward para realizar una transformación adicional en las representaciones.\n",
    "\n",
    "Cada una de estas operaciones está envuelta en una conexión residual, lo que ayuda a mantener la estabilidad del entrenamiento y evita la pérdida de información entre las capas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c7ea861080729ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:16.343228Z",
     "start_time": "2024-10-04T06:17:16.337035Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Bloque de autoatención para la secuencia de salida generada\n",
    "        self.self_attention_block = self_attention_block\n",
    "        # Bloque de atención cruzada, que toma en cuenta las representaciones del codificador\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        # Bloque feed-forward para transformar las representaciones\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        # Tres conexiones residuales: una para la autoatención, una para la atención cruzada, y una para el feed-forward\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        # Paso 1\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        # Paso 2\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        # Paso 3\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8b902",
   "metadata": {},
   "source": [
    "**Decoder**\n",
    "\n",
    "Esta representa el decodificador completo, que toma las salidas del codificador y genera la secuencia de salida objetivo, aplicando la serie de capas que incluyen autoatención y atención cruzada. Al igual que el codificador, el decodificador está compuesto por múltiples capas, y cada una de estas capas utiliza conexiones residuales para mantener la información de entrada intacta. Finalmente, se aplica una normalización de capa a la salida del decodificador.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f02f0c0fc48cf9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:18.977768Z",
     "start_time": "2024-10-04T06:17:18.971636Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        # Las capas del decodificador, una lista de DecoderBlocks\n",
    "        self.layers = layers\n",
    "        # Normalización de capa al final del decodificador\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        # Pasa la entrada (secuencia generada hasta el momento) a través de cada capa del decodificador\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        # Aplica normalización de capa al resultado final después de todas las capas\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9de153",
   "metadata": {},
   "source": [
    "**Projection Layer**\n",
    "\n",
    "Este es casi que el paso final, aqui se implementa una capa de proyección final en el transformador. Su propósito es proyectar las representaciones de salida generadas por el decodificador de la dimensión que traen al tamaño del vocabulario. Esta capa es necesaria en tareas de modelado de lenguaje, ya que convierte las representaciones del modelo en logits sobre el vocabulario, que posteriormente se pueden convertir en probabilidades para predecir la palabra siguiente en una secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "455c85041a44b369",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:21.764822Z",
     "start_time": "2024-10-04T06:17:21.757239Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        # Capa lineal para proyectar de d_model al tamaño del vocabulario\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # Proyección final: (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eafd486",
   "metadata": {},
   "source": [
    "**Transformer** (Optimus Prime)\n",
    "\n",
    "Finalmente, esta clase es la implementación completa del modelo de transformador, que incluye tanto el codificador como el decodificador. \n",
    "\n",
    "Este modelo se utiliza principalmente en tareas de secuencia a secuencia, como la traducción automática, donde la secuencia de entrada se codifica en una representación interna, y luego esa representación se usa para generar la secuencia de salida.\n",
    "\n",
    "El transformador tiene tres funciones principales:\n",
    "\n",
    "- encode: Convierte la secuencia de entrada en una representación interna utilizando el codificador.\n",
    "- decode: Genera la secuencia de salida basada en las representaciones del codificador y la secuencia objetivo.\n",
    "- project: Proyecta las representaciones finales en el espacio del vocabulario para obtener logits sobre las palabras del vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16bc6f102d1184f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:24.503055Z",
     "start_time": "2024-10-04T06:17:24.497246Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        # Codificador\n",
    "        self.encoder = encoder\n",
    "        # Decodificador\n",
    "        self.decoder = decoder\n",
    "        # Embeddings para la secuencia de input\n",
    "        self.src_embed = src_embed\n",
    "        # Embeddings para la secuencia objetivo\n",
    "        self.tgt_embed = tgt_embed\n",
    "        # Codificación posicional para la secuencia de input\n",
    "        self.src_pos = src_pos\n",
    "        # Codificación posicional para la secuencia de output\n",
    "        self.tgt_pos = tgt_pos\n",
    "        # Capa de proyección para transformar las representaciones a logits sobre el vocabulario\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # Embedding y codificación posicional para la secuencia de inputt\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        # Pasa la secuencia embebida a través del codificador\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # Embedding y codificación posicional para la secuencia de salida\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        # Pasa la secuencia embebida y la salida del codificador al decodificador\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # Proyecta las representaciones en logits sobre el vocabulario\n",
    "        # (batch, seq_len, vocab_size)\n",
    "        return self.projection_layer(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb08c9",
   "metadata": {},
   "source": [
    "**build_transformer**\n",
    "\n",
    "Aunque esta no es una de las clases, igual la quiero explicar por que es parte del proceso.\n",
    "\n",
    "Esta función construye un modelo de transformador completo, configurado para procesar secuencias de entrada y generar secuencias de salida. Tiene como parámetros el tamaño del vocabulario, las longitudes de las secuencias, el tamaño de los embeddings (d_model), el número de capas (N), el número de cabezas de atención (h), el tamaño de la red feed-forward (d_ff), y la tasa de dropout.\n",
    "\n",
    "La función define tanto el codificador como el decodificador con sus respectivos bloques de autoatención y atención cruzada, y finalmente los une en un modelo completo de transformador. Al final, también se inicializan los parámetros del transformador utilizando la inicialización de Xavier, que ayuda a mejorar la convergencia del modelo durante el entrenamiento.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:17:26.589430Z",
     "start_time": "2024-10-04T06:17:26.579319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Embeddings de la secuencia de entrada y salida\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Codificaciones posicionales para las secuencias de entrada y salida\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    # Construcción de los bloques del codificador\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        # Bloque de autoatención múltiple para el codificador\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        # Bloque feed-forward\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        # Bloque del codificador\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Construcción de los bloques del decodificador\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        # Bloque de autoatención múltiple para la secuencia de salida\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        # Bloque de atención cruzada con las salidas del codificador\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        # Bloque feed-forward\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        # Bloque del decodificador\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Ensamblar el codificador y decodificador completos\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Capa de proyección para convertir las representaciones en logits sobre el vocabulario\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Construir el modelo de transformador completo\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # Inicialización de los parámetros del modelo usando Xavier uniform para mejorar la convergencia\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd132110",
   "metadata": {},
   "source": [
    "**Referencias**\n",
    "\n",
    "Vaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing Systems. Recuperado de: https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "PyTorch. (2024) Torch.nn Documentation. Recuperado de: https://pytorch.org/docs/stable/nn.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
